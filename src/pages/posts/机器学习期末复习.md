---
author: pigstar
cover:
  alt: cover
  square: https://cdn.jsdelivr.net/gh/pigstar02/blog_img//202307312257564.png
  url: https://cdn.jsdelivr.net/gh/pigstar02/blog_img//202307312257564.png
description: ''
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: pigstar
  name: author
- content: key3, key4
  name: keywords
pubDate: 2022-12-12 09:28:50
tags:
- 机器学习
- 人工智能
theme: light
title: 机器学习期末复习
---

> 啊啊啊啊啊啊啊怎么就开学了，期末复习变开学抱佛脚
>
> 疫情是败笔

# 常见算法模型

### 一、线性回归

**什么是回归问题**

通过建模分析变量和结果之间的关系。通常用来预测某个值，如房价，天气等。回归问题大致分为线性回归和非线性回归，选择合适的回归模型对预测正确率影响很大。本质上是用一个n阶多项式去拟合一条线，使所以已知点能均匀落在这条线的附近。

1. 线性模型(Linear Model)：$y_{w}(x)=w_0+w_1x$
2. 多项式模型(Polynomial Model)： $y_{w}(x)=w_0+w_1x+w_2x^2+w_3x^3+...+w_nx^n$

<img src="https://raw.githubusercontent.com/pigstar02/blog_img/main/202302120932444.png" alt="image-20230212093255418" style="zoom: 50%;" />

**构建线性回归模型**

线性模型的参数为$y=\alpha + \beta x$，其中，$y$是预测值，$x$是我们收集到对预测值有影响的变量，而$\alpha 和 \beta$则是模型的参数，我们需要不断的调整它。

**损失函数的构建**

损失函数是用来评价一个模型的函数，我们需要不断优化这个函数的值，从而使模型最优。

- 最小二乘法
  $$
  Loss = \sum_{i=1}^n(y_i - f(x_i))^2
  $$

**损失函数求极值**

现在我们要使得Loss函数值最小，我们要通过求这个函数的极值得到模型参数$\alpha,\beta$。
$$
\begin{align}
L(\alpha,\beta)&=\sum(f(x_i)-y_i)^2\\
&=\sum(\alpha+\beta x_i - y_i)^2
\end{align}
$$
我们对上述等式左右两边分别求$\alpha 和 \beta$的偏导
$$
\left\{ 
\begin{align}
\frac{\partial}{\partial \beta}L(\alpha,\beta)&=2\sum(\alpha+\beta x_i-y_i)x_i &= 0 \\ 
&= 2(\beta\sum x_{i}^2+\sum{(\alpha - y_i)}x_i)\\
\frac{\partial}{\partial \alpha}L(\alpha,\beta)&=2\sum(\alpha+\beta x_i-y_i) &= 0 \\
&= 2(\sum\alpha+\sum{(\beta x_i - y_i)})\\

\end{align}
\right.
$$

$$
\begin{align}
令 n\bar{x} &= \sum x_i,n\bar{y} = \sum y_i \\
\alpha &= \bar{y} - \beta \bar{x} \\
因为 0 &= (\beta\sum x_{i}^2+\sum{(\alpha - y_i)}x_i)\\
0 &= \beta \sum{x_i^2} + \alpha \sum x_i - \sum x_i y_i \\
0 &= \beta \sum{x_i^2} + (\bar{y} - \beta \bar{x}) n\bar{x} - \sum x_i y_i \\
0 &= \beta \sum{x_i^2} + n\bar{x}\bar{y} - n\beta \bar{x}^2 - \sum x_i y_i \\
0 &= \beta (\sum{x_i^2} -n\bar{x}^2) + n\bar{x}\bar{y} - \sum x_i y_i \\
\end{align}
$$

$$
\left\{
\begin{align}
\beta &= \frac{\sum x_i y_i - n\bar{x}\bar{y}}{\sum{x_i^2} -n\bar{x}^2} \\
\alpha &= \bar{y} - \beta \bar{x} \\
\end{align}
\right.
$$

### 多元线性回归

$$
\left[\begin{array}{c}
Y_{1} \\
Y_{2} \\
\vdots \\
Y_{n}
\end{array}\right]=\left[\begin{array}{c}
\alpha+\beta X_{1} \\
\alpha+\beta X_{2} \\
\vdots \\
\alpha+\beta X_{n}
\end{array}\right]=\left[\begin{array}{cc}
1 & X_{1} \\
1 & X_{2} \\
\vdots & \vdots \\
1 & X_{n}
\end{array}\right] \times\left[\begin{array}{c}
\alpha \\
\beta
\end{array}\right]
$$

Y 是一个由训练实例响应变量组成的列向量。β 是一个由模型参数值组成的列向量。X 有时也被称为特征向量，是一个$m \times n$的矩阵。m是样本数，n是样本的特征数量。

**思考**

- 能否在等式的每一边都除以X来解出$\beta$值

  但是我们回想一下会发现直接除以一个矩阵是不可行的。然而，除以一个整数等同于乘以同一个整数的倒数，我们可以通过乘以矩阵X的逆矩阵来避免矩阵除法。需要注意的是只有方阵可以求逆。矩阵X并不一定是方阵。

- 我们改如何制造X的方阵出来？

  我们需要将X乘以其转置来产出一个可以求逆的方阵。

$$
\beta=\left(X^{\mathrm{T}} X\right)^{-1} X^{\mathrm{T}} Y\\
先乘转置再求逆再转置回来
$$

**复习矩阵的知识点**

- **转置**
  $$
  \left[\begin{array}{c}
  Y_{11} \space Y_{12} \space Y_{13}\\
  Y_{21} \space Y_{22} \space Y_{23}\\
  Y_{31} \space Y_{32} \space Y_{33}\\
  \end{array}\right]^{T}=\left[\begin{array}{c}
  Y_{11} \space Y_{21} \space Y_{31}\\
  Y_{12} \space Y_{22} \space Y_{32}\\
  Y_{13} \space Y_{23} \space Y_{33}\\
  \end{array}\right]
  $$

- **求逆**
  $$
  \left[\begin{array}{c}
  1 \space  2\\
  -1 \space -3\\
  \end{array}\right] \cdot \left[\begin{array}{c}
  a \space b\\
  c \space d\\
  \end{array}\right] = \left[\begin{array}{c}
  1 \space 0\\
  0 \space 1\\
  \end{array}\right] \\
  然后求解a,b,c,d
  $$
  

---

### 二、逻辑回归

**基本思想**

这个模型主要用来执行分类任务，Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计。

**单位阶跃函数**

线性回归模型预测产生的值是一个实数，我们要将它转化为0/1值

- sigmod函数
  $$
  y = \frac{1}{1+e^{-x}}
  $$

**逻辑回归分布**
$$
\begin{array}{c}
F(x)=P(X \leq x)=\frac{1}{1+e^{-(x-\mu) / \gamma}} \\
f(x)=F^{\prime}(X \leq x)=\frac{e^{-(x-\mu) / \gamma}}{\gamma\left(1+e^{-(x-\mu) / \gamma}\right)^{2}}
\end{array}
$$
$\mu$是位置参数，$\gamma$是形状参数。Sigmod函数就是逻辑分布函数在$\mu=0,\gamma=1$的特殊情况

**概率转化**
$$
y = \frac{1}{1+e^{-z}}
$$
将线性函数代入：
$$
\begin{align}
y &= \frac{1}{1+e^{-(\omega ^T x + b)}} \\
\ln \frac{y}{1-y}&=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b
\end{align}
$$
若y为样本作为正例的可能性，则1-y为样本作为反例的可能性，$\frac{y}{1-y}$则是两者的比值，$\ln \frac{y}{1-y}$叫做对数几率

**极大似然估计**

令：
$$
\begin{array}{l}
P(Y=1 \mid x)=p(x) \\
P(Y=0 \mid x)=1-p(x)
\end{array}
$$
似然函数：
$$
L(w)=\prod\left[p\left(x_{i}\right)\right]^{y_{i}}\left[1-p\left(x_{i}\right)\right]^{1-y_{i}}
$$
为了更方便求解, 我们对等式两边同取对数, 写成对数似然函数:
$$
\begin{aligned}
L(w) & =\sum\left[y_{i} \ln p\left(x_{i}\right)+\left(1-y_{i}\right) \ln \left(1-p\left(x_{i}\right)\right)\right] \\
& =\sum\left[y_{i} \ln \frac{p\left(x_{i}\right)}{1-p\left(x_{i}\right)}+\ln \left(1-p\left(x_{i}\right)\right)\right] \\
& =\sum\left[y_{i}\left(w^{T} \cdot x_{i}\right)-\ln \left(1+e^{w^{T} \cdot x_{i}}\right)\right] \\
& y_i = 0或1
\end{aligned}
$$
**平均对数似然损失**
$$
\begin{align}
J(w)&=-\frac{1}{N} \ln L(w)\\

J(w)&=-\frac{1}{n}\left(\sum_{i=1}^{n}\left(y_{i} \ln p\left(x_{i}\right)+\left(1-y_{i}\right) \ln \left(1-p\left(x_{i}\right)\right)\right)\right.
\end{align}
$$
**随机梯度下降**


$$
梯度下降是通过J(w）对 W的一阶导数来找下降方向，并且以迭代
的方式来更新参数，更新方式为\\
g_{i}=\frac{\partial J(w)}{\partial w_{i}}=(p(x_{i})-y_{i})x_{i} \\


w_{i}^{k+1}=w_{i}^{k}- \alpha g_{i}
$$
**Logistic 回归的本质**

假设数据服从这个分布，然后使用极大似然估计做参数的估计。通过上述推导我们可以看到Logistic回归实际上是使用线性回归模型的预测值逼近分类任务真实标记的对数几率，其优点有:

- 直接对分类的概率建模，无需实现假设数据分布，从而避免了假设分布不准确带来的问题（区
  别于生成式模型）

- 不仅可预测出类别，还能得到该预测的概率，这对一些利用概率辅助决策的任务很有用
- 对数几率函数是任意阶可导的凸函数，有许多数值优化算法都可以求出最优解

---

### 三、支持向量机

**一个分类器无法解决的问题该如何**

- 使用多条直线对不同区域划分，即用多个分类器一起工作

<img src="https://raw.githubusercontent.com/pigstar02/blog_img/main/202302131536995.png" alt="image-20230213153612822" style="zoom: 33%;" />

- 将数据映射到一个**高维空间**，使其线性可分

  <img src="https://raw.githubusercontent.com/pigstar02/blog_img/main/202302131537773.png" alt="image-20230213153737698" style="zoom:33%;" />

**支持向量机的基本思想**

给定训练样本集$D=\{(X1,y1),(X2，V2), ..（ Xm, ym)\}，yi∈\{-1,+1\}$,分类学习最基本的想法是基于训练集D在样本空间找到一个划分超平面，将不同类别的样本分开。

但能区分样本的超平面有很多，选**泛化能力最强**的。

**超平面的基本形式**
$$
w^{\mathrm{T}}x+b=0 \\
w是法向量，决定方向；b决定了超平面和原点的距离，改变b超平面平移
$$
**支持向量**

如下图所示，距离超平面最近的这几个训练样本点使上式的等号成立，它们被称为"支持向量"(support vector)，两个异类支持向量到超平面的距离：$ = \frac{{1}}{\vert\vert\omega\vert\vert}$

<img src="https://raw.githubusercontent.com/pigstar02/blog_img/main/202302131551349.png" alt="image-20230213155118271" style="zoom: 33%;" />

所以$\gamma = \frac{{2}}{\vert\vert\omega\vert\vert}$称为“间隔”（margin）

**最大间隔计算**

想要找到最大间隔就是找到参数使得$\gamma = \frac{{2}}{\vert\vert\omega\vert\vert}$最大
$$
同时满足

\begin{array}{c c}{ w^{T}x_{i}+b\ge+1,}&{y_{i}=+1}\\ 
{w^{T}x_{i}+b\le-1,}&{y_{i}=-1}
\end{array}
$$
仅需要最大化${\vert\vert\omega\vert\vert}^{-1}$，这等于最小化$\frac{1}{2}{\vert\vert\omega\vert\vert}^2$

**拉格朗日乘子法**

拉格朗日乘子法是一种寻求多元函数在约束条件下的极值的方法。通过引入拉格朗日乘子，可以将带约束条件的优化问题转换为不带约束的优化问题。

带约束的话则为**条件极值**，如果约束为等式，有时借助换元法可以将有条件转化为无条件极值从而求解，不过换元消元也只能解决三元以内的问题。而**拉格朗日乘数法**可以通过引入新的未知标量（**拉格朗日乘数** $\gamma $ ），直接求多元函数条件极值，不必先把问题转化为无条件极值的问题。

求函数$f(x,y)$在附加条件$\varphi(x,y) = 0$下可能的极值点
$$
L(x,y)=f(x,y)+\lambda\varphi(x,y), 其中\gamma为参数，则各阶偏导为0 \\ 
f_{x}(x,y)+ \gamma\varphi_x(x,y)= 0\\
f_{y}(x,y)+ \gamma\varphi_y(x,y)= 0\\则该方程组解出的 x,y,\gamma就是函数f(x,y)在附件条件下的所有可能极值点\\
\varphi(x,y)= 0
$$
![转载自https://zhuanlan.zhihu.com/p/74484361](https://raw.githubusercontent.com/pigstar02/blog_img/main/202302131711104.png)

**对偶问题**

简单而言，就是原先的问题是先求L对 $\lambda$ 的max再求对W、b的min，变成先求L对W、b的min再求$\lambda$的max。

将 $$W=\sum_{1}^{n} \lambda_i y_i X_i$$代入$L(W,b,\lambda)=f(W)-\sum_{1}^{n}\lambda_{i}g_{i}(W)={\frac{||W||^{2}}{2}}-\sum_{1}^{n}\lambda_{i}y_{i}(W X_{i}+b)+\sum_{1}^{n}\lambda_{i}$，得:
$$
L={\frac{1}{2}}(\sum_{1}^{n}\lambda_{i}y_{i}X_{i})(\sum_{1}^{n}\lambda_{i}y_{i}X_{i})-\sum_{1}^{n}\lambda_{i}y_{i}((\sum_{1}^{n}\lambda_{i}y_{i}X_{i})X_{i}+b)+\sum_{1}^{n}\lambda_{i} \\


=\sum_{1}^{n}\lambda_{i}-\frac{1}{2}(\sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_{i}\lambda_{j}y_{i}y_{j}X_{i}^{T}X_{j}\big)
$$
此时问题中的$w和b$被消去，这是一个关于$\lambda$的函数，求他的最大值，再反代回去即可得到我们想要的$w和b$

**核函数**

pass

---

### 四、决策树

**什么是决策树**

决策树：一棵决策树包含一个根节点，若干个内部节点和若干个叶子节点。叶结点对应决策树（样本），其他每个结点则对应于一个属性测试（**根据不同的结果走向不同的分支**）

![利用决策树算法预测西瓜的好坏_小子令狐冲-程序员宝宝_西瓜决策树 - 程序员宝宝](https://raw.githubusercontent.com/pigstar02/blog_img/main/202302132141156.png)

**划分选择**

一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度〞(purity)越来越高。

**信息熵**

用来度量样本集合纯度最常用的一种指标。假定当前样本集合D中第k 类样本所占的比例为$P_{k}(k=1,2,…，|y|)$，则D的信息熵定义为：
$$
\mathrm{Ent}(D)=-\sum_{k=1}^{|y|}p_{k}\log_{2}p_{k} \\
Ent(D)的值越小，则D的纯度越高。
$$
<u>分支是按照属性分的（颜色，纹理），但是计算纯度的时候是按照属于哪一类样本分（好瓜or坏瓜）</u>

**信息增益**

假定离散属性α分布有V个可能的取值{$$α^1,α^2,\cdot\cdot\cdot α^{V}$$} ,若使用α来对样本集 $${\mathcal{D}}$$ 进行划分，则会产生V个分支结点，其中第v个分支结点包含了D中所有在属性α上取值为$\alpha ^ V$的样本记为 ${D}^{v}$ 。再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重即**样本数越多的分支结点的影响越大**，于是可计算出用属性对样本集D进行划分所获得的“信息增益”(information gain）

**信息增益（ID3）**
$$
\mathrm{Gain}(D,\alpha)=\mathrm{Ent}(D)-\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}\,\mathrm{Ent}\,(D^{v})
$$
一般而言，**信息增益越大**，则意味着使用属性$\alpha$来进行划分所获得的“**纯度提升越大**。因此，我们可用信息增益来进行决策树的划分属性选择.著名的 ID3 决策树学习算法就是以信息增益为准则来选择划分属性。

<u>缺点:实际上，**信息增益**准则对可取值数目**较多**的**属性有所偏好**</u>

**增益率（C4.5）**

不直接使用信息增益，而是使用“增益率”(gain ratio) 来选择最优划分属性。增益率定义为：
$$
\mathrm{Gain_-\,r a t i o}(D,a)=\frac{Gain}{(D,a)}{IV(a)} \\

其中：IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}\log_{2}\frac{|D^v|}{|D|}为属性a的固有值
$$
*属性 α 的可能取值数目越多(即 V 越大)，则 IV(α) 的值通常会越大。*

注意：增益率准则对可取值数目较少的属性有所偏好。因此，C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出**信息增益高于平均水平的属性**，再从中选择**增益率最高**的。

**基尼指数（CART）**
$$
基尼值
\begin{align}
Gini(D)&=\sum_{k=1}^{|y|}\sum_{k^{\prime}\neq k}p_{k}p_{k^{\prime}}\\ 
&=1-\sum_{k=1}^{|y|}p_{k}^{2}
\end{align}
$$
直观来说，Gini(D)反映了从数据集 D 中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D) **越小**，则数据集 D的**纯度越高**
$$
基尼指数：
{Gini}_{index}(D,a)=\sum_{v=1}^{V}{\frac{|D^{v}|}{|D|}}{Gini}(D^{v})
$$
**ID3、 C4.5及CART决策树对比分析**

![image-20230214072319083](https://raw.githubusercontent.com/pigstar02/blog_img/main/202302140723209.png)

---

