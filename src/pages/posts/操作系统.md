---
author: pigstar
cover:
  alt: cover
  square: https://cdn.jsdelivr.net/gh/pigstar02/blog_img//202308011423336.jpeg
  url: https://cdn.jsdelivr.net/gh/pigstar02/blog_img//202308011423336.jpeg
description: ''
keywords: 面试, 操作系统 
layout: ../../layouts/MarkdownPost.astro
meta:
- content: pigstar
  name: author
- content: 面试, 操作系统 
  name: keywords
pubDate: 2023-08-01 14:17:00
tags:
- 面试
- 操作系统
theme: light
title: 操作系统
---
## 什么是操作系统
我的理解是本质上还是一种软件，不过拥有更高的权限，可以管理计算机硬件软件资源，为上层的软件提供服务，比如说内存管理，io操作

## 中断和异常区别
总体都是暂停当前任务，然后去执行一个设定好的处理程序。  
唯一区别是中断会把当前操作执行完，而异常则是立马跳到异常处理程序  
因为中断是从外部发生的，对当前操作并没有影响；而异常是内部执行错误发生的，本就表明当前操作无法进行下去了。  
中断执行完处理程序会回到原来的位置继续执行，而异常可能会直接结束程序。
| 中断            	|                 异常                 	|
|-----------------	|:------------------------------------:	|
| cpu时间片用完了<br>缓存区已满 	| 段错误<br>非法内存<br>除以零<br>溢出 	|

## CPU地址翻译过程
CPU地址翻译的过程就是把虚拟地址转化为真实的物理地址  
**好处：** 通过使用虚拟内存，操作系统可以更好地管理内存，每个进程都可以看到自己的连续地址空间，而不必担心物理内存的分配和管理。同时，操作系统可以更好地利用内存碎片，减少内存浪费，提高系统性能。   
虚拟内存到物理地址的映射的方法有分页和分段两种

![图源https://www.csview.cn/os/summary.html](https://pic.imgdb.cn/item/643364840d2dde57774be1c3.jpg)

### 分页系统
虚拟内存和物理内存都被划分成固定大小的一页，分页系统就像一张表（页表），记录某个物理页此时对应哪个虚拟页。

### 翻译过程
我们通过分页系统找到对应物理页是不够的，还需要一个offset记录页内偏移，毕竟一页有4KB大小  
1. 把虚拟地址分成虚拟页号和页内偏移
2. 在页表中查找虚拟页号，得到对应的物理页号
3. 把物理页号和页内偏移合成物理地址

### 多级页表
从上面的过程中可以看到最重要的一步就是通过虚拟页号查找物理页号。  
而在单级页表中，页表就是一个线性数组，索引是虚拟页号。32位计算计虚拟空间为4GB，假设一个页的大小是4KB，那么有 $2^{20}$ 个页，而每个页要4B来存储每个页对应的物理页号，那么大小就是4MB，但是这只是一个进程的，**每个进程都有一个虚拟内存空间**。于是我们引入多级页表系统。

下面以二级页表为例  

![二级页表示意图|wide](https://cdn.jsdelivr.net/gh/pigstar02/blog_img//202308021407997.png)
1. $2^{20}$ 个二级页对应所有虚拟地址
2. 再创建 $2^{10}$ 个一级页，每个一级页管理1024个二级页

从表面看，内存多出了1024个一级页。但是只有我们使用到了某个二级页，它所在的那个一级页及其管理的二级页才会被创建。  
每一个一级页未被使用就能节省4KB（它管理的1024个二级页），而多使用的内存只是1024个一级页（4KB）  
**为什么一级页表不行** ：我们始终要用页表覆盖所有虚拟内存，如果访问到没覆盖的虚拟地址，那么就无法查找物理地址，程序就无法进行下去。

## TLB页表缓存
一个进程访问同一虚拟地址肯定不止一次，而且根据局部性会在一段时间频繁访问。所以我们加入页表缓存，这样只有第一次访问需要查表翻译虚拟地址，后续的访问如果能击中缓存则不需要查表。

### 查找过程
   1. 当进行页表查询时，首先查询TLB，若查到对应虚拟地址则直接使用TLB中的物理地址。
   2. 如果没有查到则进行页表查询，把查询到的映射关系存入TLB，以备后续查询。
### TLB的特点
1. 容量小，因为每次翻译要先查找TLB，所以要具有较低的访问延迟（就是能很快知道有没有缓存）
2. 关联性
   1. 全相联：所有内存访问速度相同
   2. 组相联：TLB被分为若干个组，然后每个虚拟地址查找后根据特定规则进入相应的组，不同组有不同的查找速度、替换策略
3. 替换策略：最近最少使用（LRU）、随机替换（Random）等。

## 局部性原理
- 时间局部性：指一段时间内，一个地址被访问多次（循环）
- 空间局部性：指一段时间内，访问的地址倾向于集中在一定范围内（数组）

**应用**  
- TLB（时间）
- 高速缓存（空间：提前加载附近数据和指令）
- 多级页表（空间：访问地址集中在一定范围，有些一级页表未访问不需要分配二级页表内存）

## 用户态和内核态
首先为什么要有用户态和内核态，是为了区分不同的权限等级。  
用户态（User Mode）是程序运行时的正常状态，而内核态（Kernel Mode）是系统在执行内核代码或响应系统调用时的特权状态。

### 用户态和内核态的区别
- 权限：内核态可以访问内核地址空间，执行特权代码；用户态不行。
- 代码：执行的代码功能不同，内核态执行一些底层代码，比如文件系统，中断处理程序。
- 资源：用户态无法直接访问受保护的系统资源，比如硬件设备。

### 切换场景
1. 系统调用：当用户程序需要请求操作系统提供的服务时，会通过系统调用进入内核态。系统调用会发出一个特殊的中断，然后把CPU从用户态切换到内核态，然后执行相应的请求，执行完毕后通过中断返回指令把CPU切换回用户态。
2. 异常：发生异常会自动切换到内核态处理异常。
3. 外部中断：外部设备发出中断信号会让CPU进入内核态，执行完中断处理程序后会退回用户态

## 讲一讲CPU缓存 
一般来说缓存能提供更高的访问速度，但是缺点是存储的空间变很小。  
CPU缓存（Cache）就是位于CPU和主内存（RAM）之间的高速存储器。缓存击中可以减少CPU访问主内存的次数，从而减少消耗的时间，因为从缓存中获取比从内存中获取快得多。

### L1、L2、L3缓存
从L1到L3速度变慢，存储的数据变大（缓存命中率变大）  
这是一种层级化的存储器架构，目的是在访问速度和容量之间取得平衡。
- L1：访问速度最快，但是容量只有几十KB
- L2：访问速度较L1慢一些，容量更大，一般为几百KB或几MB。主要是弥补L1的缺点，在L1没命中时提供相对主存更快的速度。
- L3：容量一般为几MB至几十MB。在多核处理器中共享，可以在不同核之间共享数据。

### 缓存替换策略
1. 随机替换：优点是实现简单，缺点是可能会把高频访问的数据替换掉。
2. 最近最少使用（LRU）：替换最近一次使用时间距离现在最远的那条数据。实现相对复杂。
3. 最不经常使用（LFU）：替换使用频率最低的一条数据。需要跟踪每条数据的访问次数，实现较复杂。
4. 先进先出（FIFO）：就是一个队列，实现起来比较简单。但是会导致访问频繁的数据被替换。

## 缓存一致性
是多核处理器系统的关键技术，上面说到L3可以实现多核之间共享数据，那么就需要保证缓存里面的数据是一致的。

### 什么是一致性
- 写操作一致性：多个处理器同时对同一地址进行写操作时，要保证它们之间有一个明确顺序。
- 事务性：和数据库里的事务性一样，即对内存的操作要么完全执行，要么不执行，不能只执行一部分。
- 缓存一致性协议（太难不会😭）

## 伪共享问题
当多个处理器访问同一数据时，会被认为是共享，从而触发不必要的同步操作。  
但是，判定是否同一数据往往没有那么精确。因为在处理器中，数据是以缓存行为单位进行存取的，而缓存行大小一般是64字节，那么当两个处理器访问同一缓存行，不同的数据时会被误判为同一数据，即导致伪共享问题。

**后果：**
- 性能下降：频繁的同步操作。（缓存和内存之间）
- 总线流量增加：处理器核心通过总线进行数据传输（核心之间）

## 程序执行的过程
1. 编写与源代码
2. 编译/解释
   1. 编译型语言如c++/c，通过编辑器把源码编译成可执行文件（二进制文件）。编译的过程包括预处理，编译，汇编，链接等。
   2. 解释型语言如python/javascript，通过解释器逐行解释并运行。
3. 加载到内存
4. 执行指令
5. 执行过程可能会调用其他库
6. 结束后操作系统回收资源

## 常用Linux命令
- find 查找文件/路径
- pwd
- ls
- cd
- man
- grep 查找文件或输出（用管道）中符合条件的字符串
- chmod （change mode）改变权限
- ps aux --sort=-%cpu
  - ps：用于列出当前正在运行的进程的命令行实用程序。
  - aux：显示所有用户的进程，包括守护进程和其他系统进程。
  - --sort=-%cpu：按照 CPU 使用率从高到低进行排序。--sort= 表示排序的选项，- 表示降序排序，%cpu 表示按照 CPU 使用率进行排序。
- kill 进程ID 终止进程
- pkill 进程名字 杀死进程
- top 实时查看系统状态
- uptime 查看系统均衡负载
- netstat -tuln | grep :端口号 查找对应端口号的进程
  - netstat：用于显示有关网络连接的统计信息和协议状态的命令行实用程序。
  - -t：仅显示 TCP 连接。
  - -u：仅显示 UDP 连接。
  - -l：仅显示正在监听的连接。
  - -n：禁用 DNS 反向解析，以便更快地列出连接和端口号。
  - |：管道操作符，将 netstat 的输出传递给后面的命令。
  - grep：用于搜索文本的命令行实用程序。
- free 查看系统的内存使用情况
- tail -f filename
  - tail 显示文件的最后部分
  - -f f是follow的缩写，可以持续更新
  - fileame 可以是多个文件，空格隔开

## 并发和并行的区别
并发是轮流执行多个任务，从宏观上看是不同任务同时进行，其实某一时刻只有一个任务在执行。  
并行是拥有同一时间执行多个任务的能力，要求有多个CPU核心。分布式就是并行的原理，将一个任务拆分成多个小任务由多台服务器同时处理，最后再把各自的处理结果汇总。所以分布式要求任务在处理顺序不同时不影响最后的结果。  
并发能够提高系统的吞吐量和响应速度，而并行则是为了加速任务的完成速度。

## 互斥锁和自旋锁
### 互斥锁  
在多线程编程中，不同线程会竞争同一资源，这时候就需要互斥锁来保证资源同一时刻只被一个线程持有。当一个线程获得互斥锁并访问共享资源时，其他试图获得该锁的线程将被阻塞，直到锁被释放。

### 自旋锁
自旋锁用在多核系统中，与互斥锁不同的是，尝试获取互斥锁失败的线程是进入阻塞状态，即让出CPU资源；而尝试获取自旋锁失败的线程会不断循环去检查自旋锁是否可用。  
自旋锁适用于锁持有时间较短且线程不希望在等待锁时进入睡眠状态的场景。

## 死锁
### 死锁条件
1. 互斥
2. 持有并等待
3. 不可剥夺
4. 循环等待条件

### 处理死锁
#### 预防死锁
- 破坏持有并等待的条件：一次性请求所有需要的资源。

#### 避免死锁
- 银行家算法：预分配看是否会发生死锁，如果会发生则不分配资源。

#### 检测和恢复死锁
- 发生死锁时选择一个进程终止它，然后释放它持有的资源。
- 先向系统请求更多资源，执行完后再释放掉。
- 等待和重试：等待其他线程/进程执行完释放资源。

## 读写锁
允许多个线程读或者一个线程写。

**特性**
1. 共享读：它允许多个线程在不互相干扰的情况下进行并发读操作。
2. 独占写：写的时候其他线程无法获取读锁和写锁，确保共享资源不会被其他线程修改和访问。
3. 优先级：和实现方式有关。

## 条件变量
当有个线程需要等待某个条件满足时，可以通过条件变量进入休眠状态，知道另一线程改变了共享资源，然后通知条件变量唤醒一个线程，或所有线程一起竞争。

## 生产者消费者问题
就是线程池的模型。  
主线程和线程池共享一个有限容量的任务队列（共享缓存区），主进程（生产者）将数据放入队列，线程池（消费者）从队列中取出任务执行。  
问题的核心是如何实现共享缓存区的同步访问，保证数据不丢失。

1. 同步：确保生产者和消费者在正确的时间进行操作。例如，如果缓冲区已满，则生产者必须等待，直到消费者从缓冲区中取出一些数据为止。同样，如果缓冲区为空，则消费者必须等待，直到生产者将一些数据放入缓冲区为止。
2. 互斥：通过互斥锁，避免同时写入导致数据不一致

## 换页时的抖动现象
操作系统频繁缺页中断引起的现象，这个时候系统性能急剧下降，主要是在处理磁盘读写操作。

**缺页中断**就是需要的页不在内存中，需要选择一个页置换，如果这个页是脏页要先把这个页写会磁盘，再把需要的页从磁盘中读到内存中。

## 如果64位电脑是4G内存，要申请80G的空间，可以申请嘛？32位呢？
多少位电脑指的是地址总线是多少位

64位电脑的地址总线是64位，理论上能表示的虚拟地址的范围是$2^{64}$，内存空间大小约为18EB。但是会受到实际硬件的限制，如果电脑的物理内存只有4GB，那么我们只能通过内存文件映射等内存管理技术把内存映射到磁盘中，当用到时才会加载到内存中，由于磁盘和内存的速度差别过大，这就导致电脑的性能下降。

同理，32位电脑的理论最大内存空间只有4GB，所以不可能申请80GB的内存

## 共享内存时如何实现的
共享内存（Shared Memory）是一种进程间通信（IPC）机制，允许多个进程访问同一块内存区域。在共享内存的实现中，**相同的一块物理内存区域被映射到每个进程的虚拟地址空间**，从而实现数据共享。共享内存机制可以提高数据传输效率，因为它避免了数据复制和内核与用户空间之间的上下文切换。
